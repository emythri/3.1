{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0219de3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'whisper'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwhisper\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'whisper'"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import requests\n",
    "import time\n",
    "import wave\n",
    "import io\n",
    "from moviepy.editor import VideoFileClip\n",
    "from google.cloud import speech\n",
    "from jiwer import wer\n",
    "from tabulate import tabulate\n",
    "\n",
    "ASSEMBLYAI_API_KEY = 'your_assemblyai_api_key'  # Replace with your actual key\n",
    "\n",
    "# Step 1: Extract audio from video\n",
    "def extract_audio(video_path, audio_path='temp_audio.wav'):\n",
    "    video = VideoFileClip(video_path)\n",
    "    video.audio.write_audiofile(audio_path, codec='pcm_s16le')\n",
    "    return audio_path\n",
    "\n",
    "# Step 2a: Whisper transcription\n",
    "def transcribe_whisper(audio_path):\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result['text']\n",
    "\n",
    "# Step 2b: Google Speech-to-Text\n",
    "def transcribe_google(audio_path):\n",
    "    client = speech.SpeechClient()\n",
    "    with io.open(audio_path, \"rb\") as f:\n",
    "        content = f.read()\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    return \" \".join([result.alternatives[0].transcript for result in response.results])\n",
    "\n",
    "# Step 2c: AssemblyAI transcription\n",
    "def upload_to_assemblyai(audio_path):\n",
    "    headers = {'authorization': ASSEMBLYAI_API_KEY}\n",
    "    with open(audio_path, 'rb') as f:\n",
    "        response = requests.post('https://api.assemblyai.com/v2/upload', headers=headers, files={'file': f})\n",
    "    return response.json()['upload_url']\n",
    "\n",
    "def transcribe_assemblyai(audio_url):\n",
    "    headers = {'authorization': ASSEMBLYAI_API_KEY, 'content-type': 'application/json'}\n",
    "    response = requests.post('https://api.assemblyai.com/v2/transcript', headers=headers, json={\"audio_url\": audio_url})\n",
    "    transcript_id = response.json()['id']\n",
    "\n",
    "    while True:\n",
    "        polling = requests.get(f'https://api.assemblyai.com/v2/transcript/{transcript_id}', headers=headers)\n",
    "        status = polling.json()['status']\n",
    "        if status == 'completed':\n",
    "            return polling.json()['text']\n",
    "        elif status == 'error':\n",
    "            return \"Error\"\n",
    "        time.sleep(2)\n",
    "\n",
    "# Step 3: Compare all outputs\n",
    "def compare_transcriptions(reference, *others):\n",
    "    results = []\n",
    "    for name, text in others:\n",
    "        error = wer(reference, text)\n",
    "        results.append([name, f\"{(1 - error) * 100:.2f}%\", f\"{error * 100:.2f}%\"])\n",
    "    return results\n",
    "\n",
    "# Step 4: Run all\n",
    "def run_comparison(video_path):\n",
    "    print(\"üé¨ Extracting audio...\")\n",
    "    audio_path = extract_audio(video_path)\n",
    "\n",
    "    print(\"üß† Transcribing with Whisper...\")\n",
    "    whisper_text = transcribe_whisper(audio_path)\n",
    "\n",
    "    print(\"‚òÅÔ∏è Transcribing with Google STT...\")\n",
    "    google_text = transcribe_google(audio_path)\n",
    "\n",
    "    print(\"üîä Transcribing with AssemblyAI...\")\n",
    "    audio_url = upload_to_assemblyai(audio_path)\n",
    "    assembly_text = transcribe_assemblyai(audio_url)\n",
    "\n",
    "    # We'll treat Whisper output as ground truth (you can replace it with manual reference too)\n",
    "    print(\"\\nüìä Comparing outputs using Whisper as reference...\\n\")\n",
    "    results = compare_transcriptions(\n",
    "        whisper_text,\n",
    "        (\"Google STT\", google_text),\n",
    "        (\"AssemblyAI\", assembly_text)\n",
    "    )\n",
    "\n",
    "    print(tabulate(results, headers=[\"Model\", \"Accuracy\", \"WER\"], tablefmt=\"fancy_grid\"))\n",
    "\n",
    "# Example usage\n",
    "run_comparison(\"https://www.youtube.com/watch?v=kHVAk96r05Y\")  # Replace with your video path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mycvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
